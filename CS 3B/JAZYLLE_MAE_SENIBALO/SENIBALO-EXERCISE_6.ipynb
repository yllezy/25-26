{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b19b3dff",
   "metadata": {},
   "source": [
    "# Muffin vs Chihuahua Classification (Custom Dataset)\n",
    "\n",
    "This section demonstrates binary image classification using a custom muffin vs chihuahua dataset and a CNN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6904d94a",
   "metadata": {},
   "source": [
    "This is an example of a simple CNN developed, trained and utilized\n",
    "\n",
    "AI was used to help generate the codebase\n",
    "\n",
    "Note: Make sure that the tensorflow package is installed in your device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a020eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tensorflow in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (6.33.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (10.4.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e12702",
   "metadata": {},
   "source": [
    "**Section: Data — Dataset Directory Configuration**\n",
    "\n",
    "This cell defines the dataset directories used for Muffin vs Chihuahua. Update the paths if your data is located elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35c8ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lib imports\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf51c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET DIRECTORY CONFIGURATION\n",
    "# Download and unzip the dataset from Kaggle, set the directory paths accordingly.\n",
    "\n",
    "train_1 = r\"C:\\Users\\LENOVO\\Downloads\\train_split\"\n",
    "test_1 = r\"C:\\Users\\LENOVO\\Downloads\\test_split\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522d74fc",
   "metadata": {},
   "source": [
    "**Section: Data Preprocessing & Augmentation**\n",
    "\n",
    "This cell contains the ImageDataGenerator configurations and creates training, validation and test generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed9e2275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Split dataset into train/test folders (run once)\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "def split_dataset(source_dir, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split dataset into train and test directories\n",
    "    source_dir: Path to folder containing class subfolders (chihuahua, muffin)\n",
    "    train_ratio: Percentage of data for training (0.8 = 80%)\n",
    "    \"\"\"\n",
    "    source = Path(source_dir)\n",
    "    parent = source.parent\n",
    "    \n",
    "    # Create train and validation directories (avoid naming conflict)\n",
    "    train_dir = parent / \"train_split\"\n",
    "    test_dir = parent / \"test_split\"\n",
    "    \n",
    "    # Get all class folders (chihuahua, muffin)\n",
    "    for class_folder in source.iterdir():\n",
    "        if class_folder.is_dir():\n",
    "            class_name = class_folder.name\n",
    "            \n",
    "            # Create class folders in train and test\n",
    "            (train_dir / class_name).mkdir(parents=True, exist_ok=True)\n",
    "            (test_dir / class_name).mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Get all images in this class\n",
    "            images = list(class_folder.glob(\"*.*\"))\n",
    "            random.shuffle(images)\n",
    "            \n",
    "            # Split images\n",
    "            split_idx = int(len(images) * train_ratio)\n",
    "            train_images = images[:split_idx]\n",
    "            test_images = images[split_idx:]\n",
    "            \n",
    "            # Copy images to train folder\n",
    "            for img in train_images:\n",
    "                shutil.copy2(img, train_dir / class_name / img.name)\n",
    "            \n",
    "            # Copy images to test folder\n",
    "            for img in test_images:\n",
    "                shutil.copy2(img, test_dir / class_name / img.name)\n",
    "            \n",
    "            print(f\"{class_name}: {len(train_images)} train, {len(test_images)} test\")\n",
    "    \n",
    "    print(f\"\\nDataset split complete!\")\n",
    "    print(f\"Train directory: {train_dir}\")\n",
    "    print(f\"Test directory: {test_dir}\")\n",
    "\n",
    "# Remove the hash and run once to split the dataset\n",
    "# split_dataset(r\"C:\\Users\\LENOVO\\Downloads\\test\", train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef4f9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE PARAMETERS\n",
    "# Used to resize the input images, also will determine the input size of your input layer.\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d350739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 758 images belonging to 3 classes.\n",
      "Found 189 images belonging to 3 classes.\n",
      "Found 189 images belonging to 3 classes.\n",
      "Found 237 images belonging to 3 classes.\n",
      "Found 237 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING & AUGMENTATION\n",
    "# Optional but recommended for image processing tasks, especially with limited data.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_1,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_1,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_1,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f4b1252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED CNN MODEL ARCHITECTURE WITH REGULARIZATION AND DROPOUT\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "                kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71dcbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model optimizers, loss function, and metrics\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # old\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "750c313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 0.5277 - loss: -10003.2852 - val_accuracy: 0.5397 - val_loss: -63448.3008\n",
      "Epoch 2/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 0.5277 - loss: -10003.2852 - val_accuracy: 0.5397 - val_loss: -63448.3008\n",
      "Epoch 2/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -833711.8750 - val_accuracy: 0.5397 - val_loss: -2888982.2500\n",
      "Epoch 3/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -833711.8750 - val_accuracy: 0.5397 - val_loss: -2888982.2500\n",
      "Epoch 3/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -12075809.0000 - val_accuracy: 0.5397 - val_loss: -34206580.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -12075809.0000 - val_accuracy: 0.5397 - val_loss: -34206580.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -97416656.0000 - val_accuracy: 0.5397 - val_loss: -212706944.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -97416656.0000 - val_accuracy: 0.5397 - val_loss: -212706944.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -469891744.0000 - val_accuracy: 0.5397 - val_loss: -900343360.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -469891744.0000 - val_accuracy: 0.5397 - val_loss: -900343360.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -1643722112.0000 - val_accuracy: 0.5397 - val_loss: -2897585408.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -1643722112.0000 - val_accuracy: 0.5397 - val_loss: -2897585408.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -4673018880.0000 - val_accuracy: 0.5397 - val_loss: -7675681792.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -4673018880.0000 - val_accuracy: 0.5397 - val_loss: -7675681792.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -11599169536.0000 - val_accuracy: 0.5397 - val_loss: -17791905792.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -11599169536.0000 - val_accuracy: 0.5397 - val_loss: -17791905792.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -25298470912.0000 - val_accuracy: 0.5397 - val_loss: -36878561280.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -25298470912.0000 - val_accuracy: 0.5397 - val_loss: -36878561280.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -50425647104.0000 - val_accuracy: 0.5397 - val_loss: -69903024128.0000\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -50425647104.0000 - val_accuracy: 0.5397 - val_loss: -69903024128.0000\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE CNN\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7541833a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 972ms/step - accuracy: 0.5401 - loss: -64797044736.0000\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 972ms/step - accuracy: 0.5401 - loss: -64797044736.0000\n",
      "Test Accuracy: 0.5400843620300293\n",
      "Test Accuracy: 0.5400843620300293\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE THE MODEL\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ad7d399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# SAVE THE IMPROVED MODEL\n",
    "model.save('exercise_6_trained_model_improved.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45472d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE INFERENCE SCRIPT\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def predict_image(img_path, model_path='exercise_6_trained_model_improved.h5'):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    img = image.load_img(img_path, target_size=IMG_SIZE)\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    pred = model.predict(img_array)[0,0]\n",
    "    label = \"Chihuahua\" if pred >= 0.5 else \"Muffin\"\n",
    "    print(f\"Prediction: {label} (confidence: {pred:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b340f1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372ms/step\n",
      "Prediction: Chihuahua (confidence: 1.00)\n",
      "Prediction: Chihuahua (confidence: 1.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "Prediction: Chihuahua (confidence: 1.00)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "Prediction: Chihuahua (confidence: 1.00)\n"
     ]
    }
   ],
   "source": [
    "# Example usage - Run 1 and 2 prediction and confidence:\n",
    "predict_image(r\"C:\\Users\\LENOVO\\Downloads\\test_split\\chihuahua\\img_0_1082.jpg\", model_path='exercise_6_trained_model_improved.h5')\n",
    "predict_image(r\"C:\\Users\\LENOVO\\Downloads\\test_split\\muffin\\img_0_423.jpg\", model_path='exercise_6_trained_model_improved.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18dee3",
   "metadata": {},
   "source": [
    "# Cats vs Dogs Classification (Kaggle)\n",
    "\n",
    "This section demonstrates how to use the Kaggle Cats vs Dogs dataset with the improved CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fafc0ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET DIRECTORY CONFIGURATION (Cats vs Dogs)\n",
    "catsdogs_train_dir = r\"C:\\Users\\LENOVO\\Downloads\\cats_and_dogs\\training_set\\training_set\"\n",
    "catsdogs_test_dir = r\"C:\\Users\\LENOVO\\Downloads\\cats_and_dogs\\test_set\\test_set\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87276d8b",
   "metadata": {},
   "source": [
    "**Section: Model Architecture (Custom CNN)**\n",
    "\n",
    "This cell defines the improved custom CNN architecture with L2 regularization and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bde72e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6404 images belonging to 2 classes.\n",
      "Found 1601 images belonging to 2 classes.\n",
      "Found 1601 images belonging to 2 classes.\n",
      "Found 2023 images belonging to 2 classes.\n",
      "Found 2023 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING & AUGMENTATION (Cats vs Dogs)\n",
    "catsdogs_img_size = (128, 128)\n",
    "catsdogs_batch_size = 32\n",
    "catsdogs_train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "catsdogs_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "catsdogs_train_generator = catsdogs_train_datagen.flow_from_directory(\n",
    "    catsdogs_train_dir,\n",
    "    target_size=catsdogs_img_size,\n",
    "    batch_size=catsdogs_batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "catsdogs_val_generator = catsdogs_train_datagen.flow_from_directory(\n",
    "    catsdogs_train_dir,\n",
    "    target_size=catsdogs_img_size,\n",
    "    batch_size=catsdogs_batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "catsdogs_test_generator = catsdogs_test_datagen.flow_from_directory(\n",
    "    catsdogs_test_dir,\n",
    "    target_size=catsdogs_img_size,\n",
    "    batch_size=catsdogs_batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be03860",
   "metadata": {},
   "source": [
    "**Section: Model Compilation**\n",
    "\n",
    "This cell compiles the model with optimizer, loss and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "919dd51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED CNN MODEL FOR CATS VS DOGS\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "catsdogs_model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(catsdogs_img_size[0], catsdogs_img_size[1], 3),\n",
    "                kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "catsdogs_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79900cb0",
   "metadata": {},
   "source": [
    "**Section: Training**\n",
    "\n",
    "This cell runs the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e09b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 2s/step - accuracy: 0.4983 - loss: 0.8998 - val_accuracy: 0.5003 - val_loss: 0.7864\n",
      "Epoch 2/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 2s/step - accuracy: 0.4983 - loss: 0.8998 - val_accuracy: 0.5003 - val_loss: 0.7864\n",
      "Epoch 2/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 845ms/step - accuracy: 0.5139 - loss: 0.7565 - val_accuracy: 0.4984 - val_loss: 0.7394\n",
      "Epoch 3/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 845ms/step - accuracy: 0.5139 - loss: 0.7565 - val_accuracy: 0.4984 - val_loss: 0.7394\n",
      "Epoch 3/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 749ms/step - accuracy: 0.5166 - loss: 0.7260 - val_accuracy: 0.5690 - val_loss: 0.7096\n",
      "Epoch 4/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 749ms/step - accuracy: 0.5166 - loss: 0.7260 - val_accuracy: 0.5690 - val_loss: 0.7096\n",
      "Epoch 4/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 736ms/step - accuracy: 0.5384 - loss: 0.7108 - val_accuracy: 0.5403 - val_loss: 0.7093\n",
      "Epoch 5/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 736ms/step - accuracy: 0.5384 - loss: 0.7108 - val_accuracy: 0.5403 - val_loss: 0.7093\n",
      "Epoch 5/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 738ms/step - accuracy: 0.5590 - loss: 0.6937 - val_accuracy: 0.5578 - val_loss: 0.6873\n",
      "Epoch 6/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 738ms/step - accuracy: 0.5590 - loss: 0.6937 - val_accuracy: 0.5578 - val_loss: 0.6873\n",
      "Epoch 6/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 737ms/step - accuracy: 0.5767 - loss: 0.6829 - val_accuracy: 0.6021 - val_loss: 0.6747\n",
      "Epoch 7/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 737ms/step - accuracy: 0.5767 - loss: 0.6829 - val_accuracy: 0.6021 - val_loss: 0.6747\n",
      "Epoch 7/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 734ms/step - accuracy: 0.5990 - loss: 0.6717 - val_accuracy: 0.6152 - val_loss: 0.6658\n",
      "Epoch 8/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 734ms/step - accuracy: 0.5990 - loss: 0.6717 - val_accuracy: 0.6152 - val_loss: 0.6658\n",
      "Epoch 8/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 773ms/step - accuracy: 0.6240 - loss: 0.6631 - val_accuracy: 0.6315 - val_loss: 0.6583\n",
      "Epoch 9/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 773ms/step - accuracy: 0.6240 - loss: 0.6631 - val_accuracy: 0.6315 - val_loss: 0.6583\n",
      "Epoch 9/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 774ms/step - accuracy: 0.6230 - loss: 0.6592 - val_accuracy: 0.6408 - val_loss: 0.6453\n",
      "Epoch 10/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 774ms/step - accuracy: 0.6230 - loss: 0.6592 - val_accuracy: 0.6408 - val_loss: 0.6453\n",
      "Epoch 10/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 739ms/step - accuracy: 0.6455 - loss: 0.6497 - val_accuracy: 0.6596 - val_loss: 0.6250\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 739ms/step - accuracy: 0.6455 - loss: 0.6497 - val_accuracy: 0.6596 - val_loss: 0.6250\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 744ms/step - accuracy: 0.6826 - loss: 0.6133\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 744ms/step - accuracy: 0.6826 - loss: 0.6133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cats vs Dogs Test Accuracy: 0.682649552822113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "Prediction: Cat (confidence: 0.33)\n",
      "Prediction: Cat (confidence: 0.33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
      "Prediction: Dog (confidence: 0.61)\n",
      "Prediction: Dog (confidence: 0.61)\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE CNN (Cats vs Dogs)\n",
    "catsdogs_history = catsdogs_model.fit(\n",
    "    catsdogs_train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=catsdogs_val_generator\n",
    ")\n",
    "\n",
    "# EVALUATE THE MODEL (Cats vs Dogs)\n",
    "catsdogs_test_loss, catsdogs_test_acc = catsdogs_model.evaluate(catsdogs_test_generator)\n",
    "print(f\"Cats vs Dogs Test Accuracy: {catsdogs_test_acc}\")\n",
    "\n",
    "# SAVE THE IMPROVED MODEL (Cats vs Dogs)\n",
    "catsdogs_model.save('exercise_6_custom_senibalo.h5')\n",
    "\n",
    "# SIMPLE INFERENCE SCRIPT (Cats vs Dogs)\n",
    "def catsdogs_predict_image(img_path, model_path='exercise_6_custom_senibalo.h5'):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    img = image.load_img(img_path, target_size=catsdogs_img_size)\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    pred = model.predict(img_array)[0,0]\n",
    "    label = \"Dog\" if pred >= 0.5 else \"Cat\"\n",
    "    print(f\"Prediction: {label} (confidence: {pred:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afae2e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002AF10213EC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002AF10213EC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n",
      "Prediction: Cat (confidence: 0.33)\n",
      "Prediction: Cat (confidence: 0.33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002AF10302F20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002AF10302F20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "Prediction: Dog (confidence: 0.61)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "Prediction: Dog (confidence: 0.61)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "catsdogs_predict_image(r\"C:\\Users\\LENOVO\\Downloads\\cats_and_dogs\\test_set\\test_set\\cats\\cat.4975.jpg\")\n",
    "catsdogs_predict_image(r\"C:\\Users\\LENOVO\\Downloads\\cats_and_dogs\\test_set\\test_set\\dogs\\dog.4942.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ecc7e",
   "metadata": {},
   "source": [
    "**Section: Evaluation**\n",
    "\n",
    "This cell evaluates the trained model on the test set and prints accuracy (answer location for a)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237f0284",
   "metadata": {},
   "source": [
    "# Muffin vs Chihuahua — ResNet CNN Architecture\n",
    "\n",
    "This section uses ResNet50 (transfer learning) on the Muffin vs Chihuahua dataset. It performs preprocessing suited for ResNet, trains a transfer-learning model, evaluates it, and saves the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcadd538",
   "metadata": {},
   "source": [
    "**Section: ResNet (Model, Training, Eval, Save, Inference)**\n",
    "\n",
    "This section contains the ResNet50 transfer-learning workflow for Muffin vs Chihuahua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3679944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESNET50: imports & parameters\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# ResNet expects 224x224 images\n",
    "resnet_img_size = (224, 224)\n",
    "resnet_batch_size = 32\n",
    "\n",
    "# Use a smaller batch for limited GPU/CPU memory if needed\n",
    "# resnet_batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e56dbbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 758 images belonging to 3 classes.\n",
      "Found 189 images belonging to 3 classes.\n",
      "Found 189 images belonging to 3 classes.\n",
      "Found 237 images belonging to 3 classes.\n",
      "Found 237 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# RESNET50: data generators (preprocessing suited for ResNet)\n",
    "resnet_train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.resnet.preprocess_input,\n",
    "    validation_split=0.2\n",
    ")\n",
    "resnet_test_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.resnet.preprocess_input)\n",
    "\n",
    "resnet_train_generator = resnet_train_datagen.flow_from_directory(\n",
    "    train_1,\n",
    "    target_size=resnet_img_size,\n",
    "    batch_size=resnet_batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "resnet_val_generator = resnet_train_datagen.flow_from_directory(\n",
    "    train_1,\n",
    "    target_size=resnet_img_size,\n",
    "    batch_size=resnet_batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "resnet_test_generator = resnet_test_datagen.flow_from_directory(\n",
    "    test_1,\n",
    "    target_size=resnet_img_size,\n",
    "    batch_size=resnet_batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3df5b462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1us/step\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "# RESNET50: build model (feature extraction)\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(resnet_img_size[0], resnet_img_size[1], 3)\n",
    ")\n",
    "base_model.trainable = False  # freeze base for feature extraction\n",
    "\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "resnet_model = models.Model(inputs=base_model.input, outputs=preds)\n",
    "resnet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a770599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 5s/step - accuracy: 0.5172 - loss: -2.7920 - val_accuracy: 0.5397 - val_loss: -6.8877\n",
      "Epoch 2/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 5s/step - accuracy: 0.5172 - loss: -2.7920 - val_accuracy: 0.5397 - val_loss: -6.8877\n",
      "Epoch 2/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 4s/step - accuracy: 0.5409 - loss: -10.8777 - val_accuracy: 0.5397 - val_loss: -15.7648\n",
      "Epoch 3/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 4s/step - accuracy: 0.5409 - loss: -10.8777 - val_accuracy: 0.5397 - val_loss: -15.7648\n",
      "Epoch 3/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 4s/step - accuracy: 0.5409 - loss: -19.4120 - val_accuracy: 0.5397 - val_loss: -25.2013\n",
      "Epoch 4/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 4s/step - accuracy: 0.5409 - loss: -19.4120 - val_accuracy: 0.5397 - val_loss: -25.2013\n",
      "Epoch 4/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 4s/step - accuracy: 0.5409 - loss: -28.7135 - val_accuracy: 0.5397 - val_loss: -35.1357\n",
      "Epoch 5/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 4s/step - accuracy: 0.5409 - loss: -28.7135 - val_accuracy: 0.5397 - val_loss: -35.1357\n",
      "Epoch 5/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 4s/step - accuracy: 0.5409 - loss: -38.8033 - val_accuracy: 0.5397 - val_loss: -45.5631\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 4s/step - accuracy: 0.5409 - loss: -38.8033 - val_accuracy: 0.5397 - val_loss: -45.5631\n"
     ]
    }
   ],
   "source": [
    "# RESNET50: train (few epochs for demo; increase for better performance)\n",
    "resnet_history = resnet_model.fit(\n",
    "    resnet_train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=resnet_val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff2a980f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3s/step - accuracy: 0.5401 - loss: -43.8318\n",
      "ResNet Test Accuracy: 0.5400843620300293\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3s/step - accuracy: 0.5401 - loss: -43.8318\n",
      "ResNet Test Accuracy: 0.5400843620300293\n"
     ]
    }
   ],
   "source": [
    "# RESNET50: evaluate, save, and example predictions\n",
    "# Evaluate and mark the answer location for\n",
    "resnet_test_loss, resnet_test_acc = resnet_model.evaluate(resnet_test_generator)\n",
    "print(f\"ResNet Test Accuracy: {resnet_test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0bc1c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the ResNet model and mark location for\n",
    "resnet_model.save('exercise_6_trained_resnet50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cce560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_80\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(1, 128, 128, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Predictions for Run 1 and Run 2.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predict_image(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLENOVO\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest_split\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mchihuahua\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimg_1_153.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexercise_6_trained_resnet50.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m predict_image(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLENOVO\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest_split\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmuffin\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimg_0_423.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexercise_6_trained_resnet50.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m, in \u001b[0;36mpredict_image\u001b[1;34m(img_path, model_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m img_array \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mimg_to_array(img) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m      8\u001b[0m img_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img_array, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img_array)[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     10\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChihuahua\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMuffin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (confidence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"functional_80\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(1, 128, 128, 3)"
     ]
    }
   ],
   "source": [
    "# RESNET: single-image prediction helper (uses ResNet preprocessing and correct input size)\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def predict_image_resnet(img_path, model_path='exercise_6_trained_resnet50.h5'):\n",
    "    \"\"\"Load an image, resize to ResNet input size, apply preprocess_input, and predict.\"\"\"\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    img = image.load_img(img_path, target_size=resnet_img_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    # Use ResNet's preprocess_input (handles scaling/normalization expected by the pretrained model)\n",
    "    img_array = tf.keras.applications.resnet.preprocess_input(img_array)\n",
    "    pred = model.predict(img_array)[0,0]\n",
    "    # For this binary setup: label mapping (consistent with training) -- adjust threshold if needed\n",
    "    label = 'Muffin' if pred >= 0.5 else 'Chihuahua'\n",
    "    print(f\"Prediction: {label} (raw score: {pred:.4f})\")\n",
    "\n",
    "# Predictions for Run 1 and Run 2 using the ResNet model\n",
    "predict_image_resnet(r\"C:\\Users\\LENOVO\\Downloads\\test_split\\chihuahua\\img_1_153.jpg\", model_path='exercise_6_trained_resnet50.h5')\n",
    "predict_image_resnet(r\"C:\\Users\\LENOVO\\Downloads\\test_split\\muffin\\img_0_423.jpg\", model_path='exercise_6_trained_resnet50.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b578013",
   "metadata": {},
   "source": [
    "**Section: Cats vs Dogs (Dataset header)**\n",
    "\n",
    "This section contains the Cats vs Dogs workflow using the same improved CNN architecture for comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
