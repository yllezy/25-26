{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b19b3dff",
   "metadata": {},
   "source": [
    "# Muffin vs Chihuahua Classification (Custom Dataset)\n",
    "\n",
    "This section demonstrates binary image classification using a custom muffin vs chihuahua dataset and a CNN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6904d94a",
   "metadata": {},
   "source": [
    "This is an example of a simple CNN developed, trained and utilized\n",
    "\n",
    "AI was used to help generate the codebase\n",
    "\n",
    "Note: Make sure that the tensorflow package is installed in your device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5a020eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tensorflow in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (6.33.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (10.4.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "35c8ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lib imports\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6cf51c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET DIRECTORY CONFIGURATION\n",
    "# Download and unzip the dataset from Kaggle, set the directory paths accordingly.\n",
    "\n",
    "train_1 = r\"C:\\Users\\LENOVO\\Downloads\\train_split\"\n",
    "test_1 = r\"C:\\Users\\LENOVO\\Downloads\\test_split\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ed9e2275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Split dataset into train/test folders (run once)\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "def split_dataset(source_dir, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split dataset into train and test directories\n",
    "    source_dir: Path to folder containing class subfolders (chihuahua, muffin)\n",
    "    train_ratio: Percentage of data for training (0.8 = 80%)\n",
    "    \"\"\"\n",
    "    source = Path(source_dir)\n",
    "    parent = source.parent\n",
    "    \n",
    "    # Create train and validation directories (avoid naming conflict)\n",
    "    train_dir = parent / \"train_split\"\n",
    "    test_dir = parent / \"test_split\"\n",
    "    \n",
    "    # Get all class folders (chihuahua, muffin)\n",
    "    for class_folder in source.iterdir():\n",
    "        if class_folder.is_dir():\n",
    "            class_name = class_folder.name\n",
    "            \n",
    "            # Create class folders in train and test\n",
    "            (train_dir / class_name).mkdir(parents=True, exist_ok=True)\n",
    "            (test_dir / class_name).mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Get all images in this class\n",
    "            images = list(class_folder.glob(\"*.*\"))\n",
    "            random.shuffle(images)\n",
    "            \n",
    "            # Split images\n",
    "            split_idx = int(len(images) * train_ratio)\n",
    "            train_images = images[:split_idx]\n",
    "            test_images = images[split_idx:]\n",
    "            \n",
    "            # Copy images to train folder\n",
    "            for img in train_images:\n",
    "                shutil.copy2(img, train_dir / class_name / img.name)\n",
    "            \n",
    "            # Copy images to test folder\n",
    "            for img in test_images:\n",
    "                shutil.copy2(img, test_dir / class_name / img.name)\n",
    "            \n",
    "            print(f\"{class_name}: {len(train_images)} train, {len(test_images)} test\")\n",
    "    \n",
    "    print(f\"\\nDataset split complete!\")\n",
    "    print(f\"Train directory: {train_dir}\")\n",
    "    print(f\"Test directory: {test_dir}\")\n",
    "\n",
    "# Remove the hash and run once to split the dataset\n",
    "# split_dataset(r\"C:\\Users\\LENOVO\\Downloads\\test\", train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ef4f9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE PARAMETERS\n",
    "# Used to resize the input images, also will determine the input size of your input layer.\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d350739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 758 images belonging to 3 classes.\n",
      "Found 189 images belonging to 3 classes.\n",
      "Found 189 images belonging to 3 classes.\n",
      "Found 237 images belonging to 3 classes.\n",
      "Found 237 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING & AUGMENTATION\n",
    "# Optional but recommended for image processing tasks, especially with limited data.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_1,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_1,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_1,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3f4b1252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED CNN MODEL ARCHITECTURE WITH REGULARIZATION AND DROPOUT\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "                kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "71dcbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model optimizers, loss function, and metrics\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # old\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "750c313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - accuracy: 0.5343 - loss: -12116.8184 - val_accuracy: 0.5397 - val_loss: -70929.8281\n",
      "Epoch 2/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - accuracy: 0.5343 - loss: -12116.8184 - val_accuracy: 0.5397 - val_loss: -70929.8281\n",
      "Epoch 2/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -898319.1875 - val_accuracy: 0.5397 - val_loss: -3183275.7500\n",
      "Epoch 3/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -898319.1875 - val_accuracy: 0.5397 - val_loss: -3183275.7500\n",
      "Epoch 3/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -14285393.0000 - val_accuracy: 0.5397 - val_loss: -38399440.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -14285393.0000 - val_accuracy: 0.5397 - val_loss: -38399440.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -109683632.0000 - val_accuracy: 0.5397 - val_loss: -241253744.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -109683632.0000 - val_accuracy: 0.5397 - val_loss: -241253744.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -527785056.0000 - val_accuracy: 0.5397 - val_loss: -1013165952.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -527785056.0000 - val_accuracy: 0.5397 - val_loss: -1013165952.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -1854560384.0000 - val_accuracy: 0.5397 - val_loss: -3259995392.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -1854560384.0000 - val_accuracy: 0.5397 - val_loss: -3259995392.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -5428987392.0000 - val_accuracy: 0.5397 - val_loss: -8733277184.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -5428987392.0000 - val_accuracy: 0.5397 - val_loss: -8733277184.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -13170657280.0000 - val_accuracy: 0.5397 - val_loss: -20030277632.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -13170657280.0000 - val_accuracy: 0.5397 - val_loss: -20030277632.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -28779579392.0000 - val_accuracy: 0.5397 - val_loss: -41501032448.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -28779579392.0000 - val_accuracy: 0.5397 - val_loss: -41501032448.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -56839872512.0000 - val_accuracy: 0.5397 - val_loss: -78479048704.0000\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.5409 - loss: -56839872512.0000 - val_accuracy: 0.5397 - val_loss: -78479048704.0000\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE CNN\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7541833a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.5401 - loss: -72706793472.0000\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.5401 - loss: -72706793472.0000\n",
      "Test Accuracy: 0.5400843620300293\n",
      "Test Accuracy: 0.5400843620300293\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE THE MODEL\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2ad7d399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# SAVE THE IMPROVED MODEL\n",
    "model.save('exercise_6_trained_model_improved.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "45472d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE INFERENCE SCRIPT\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def predict_image(img_path, model_path='exercise_6_trained_model_improved.h5'):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    img = image.load_img(img_path, target_size=IMG_SIZE)\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    pred = model.predict(img_array)[0,0]\n",
    "    label = \"Chihuahua\" if pred >= 0.5 else \"Muffin\"\n",
    "    print(f\"Prediction: {label} (confidence: {pred:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b340f1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
      "Prediction: Chihuahua (confidence: 1.00)\n",
      "Prediction: Chihuahua (confidence: 1.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "Prediction: Chihuahua (confidence: 1.00)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "Prediction: Chihuahua (confidence: 1.00)\n"
     ]
    }
   ],
   "source": [
    "# Example usage - Run 1 and 2 prediction and confidence:\n",
    "predict_image(r\"C:\\Users\\LENOVO\\Downloads\\test_split\\chihuahua\\img_0_1082.jpg\", model_path='exercise_6_trained_model_improved.h5')\n",
    "predict_image(r\"C:\\Users\\LENOVO\\Downloads\\test_split\\muffin\\img_0_423.jpg\", model_path='exercise_6_trained_model_improved.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18dee3",
   "metadata": {},
   "source": [
    "# Cats vs Dogs Classification (Kaggle)\n",
    "\n",
    "This section demonstrates how to use the Kaggle Cats vs Dogs dataset with the improved CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fafc0ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET DIRECTORY CONFIGURATION (Cats vs Dogs)\n",
    "catsdogs_train_dir = r\"C:\\Users\\LENOVO\\Downloads\\cats_and_dogs\\training_set\\training_set\"\n",
    "catsdogs_test_dir = r\"C:\\Users\\LENOVO\\Downloads\\cats_and_dogs\\test_set\\test_set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9bde72e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6404 images belonging to 2 classes.\n",
      "Found 1601 images belonging to 2 classes.\n",
      "Found 1601 images belonging to 2 classes.\n",
      "Found 2023 images belonging to 2 classes.\n",
      "Found 2023 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING & AUGMENTATION (Cats vs Dogs)\n",
    "catsdogs_img_size = (128, 128)\n",
    "catsdogs_batch_size = 32\n",
    "catsdogs_train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "catsdogs_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "catsdogs_train_generator = catsdogs_train_datagen.flow_from_directory(\n",
    "    catsdogs_train_dir,\n",
    "    target_size=catsdogs_img_size,\n",
    "    batch_size=catsdogs_batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "catsdogs_val_generator = catsdogs_train_datagen.flow_from_directory(\n",
    "    catsdogs_train_dir,\n",
    "    target_size=catsdogs_img_size,\n",
    "    batch_size=catsdogs_batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "catsdogs_test_generator = catsdogs_test_datagen.flow_from_directory(\n",
    "    catsdogs_test_dir,\n",
    "    target_size=catsdogs_img_size,\n",
    "    batch_size=catsdogs_batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "919dd51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED CNN MODEL FOR CATS VS DOGS\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "catsdogs_model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(catsdogs_img_size[0], catsdogs_img_size[1], 3),\n",
    "                kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "catsdogs_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "90e09b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 1s/step - accuracy: 0.5094 - loss: 0.8807 - val_accuracy: 0.4997 - val_loss: 0.7773\n",
      "Epoch 2/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 1s/step - accuracy: 0.5094 - loss: 0.8807 - val_accuracy: 0.4997 - val_loss: 0.7773\n",
      "Epoch 2/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 618ms/step - accuracy: 0.5020 - loss: 0.7508 - val_accuracy: 0.5428 - val_loss: 0.7329\n",
      "Epoch 3/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 618ms/step - accuracy: 0.5020 - loss: 0.7508 - val_accuracy: 0.5428 - val_loss: 0.7329\n",
      "Epoch 3/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 614ms/step - accuracy: 0.5217 - loss: 0.7219 - val_accuracy: 0.5147 - val_loss: 0.7170\n",
      "Epoch 4/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 614ms/step - accuracy: 0.5217 - loss: 0.7219 - val_accuracy: 0.5147 - val_loss: 0.7170\n",
      "Epoch 4/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 2s/step - accuracy: 0.5231 - loss: 0.7119 - val_accuracy: 0.5628 - val_loss: 0.7020\n",
      "Epoch 5/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 2s/step - accuracy: 0.5231 - loss: 0.7119 - val_accuracy: 0.5628 - val_loss: 0.7020\n",
      "Epoch 5/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 704ms/step - accuracy: 0.5484 - loss: 0.6995 - val_accuracy: 0.5696 - val_loss: 0.6911\n",
      "Epoch 6/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 704ms/step - accuracy: 0.5484 - loss: 0.6995 - val_accuracy: 0.5696 - val_loss: 0.6911\n",
      "Epoch 6/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 704ms/step - accuracy: 0.5628 - loss: 0.6909 - val_accuracy: 0.5597 - val_loss: 0.6852\n",
      "Epoch 7/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 704ms/step - accuracy: 0.5628 - loss: 0.6909 - val_accuracy: 0.5597 - val_loss: 0.6852\n",
      "Epoch 7/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 699ms/step - accuracy: 0.5637 - loss: 0.6873 - val_accuracy: 0.5215 - val_loss: 0.7020\n",
      "Epoch 8/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 699ms/step - accuracy: 0.5637 - loss: 0.6873 - val_accuracy: 0.5215 - val_loss: 0.7020\n",
      "Epoch 8/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 702ms/step - accuracy: 0.5611 - loss: 0.6869 - val_accuracy: 0.5403 - val_loss: 0.6928\n",
      "Epoch 9/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 702ms/step - accuracy: 0.5611 - loss: 0.6869 - val_accuracy: 0.5403 - val_loss: 0.6928\n",
      "Epoch 9/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 725ms/step - accuracy: 0.5792 - loss: 0.6769 - val_accuracy: 0.5765 - val_loss: 0.6737\n",
      "Epoch 10/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 725ms/step - accuracy: 0.5792 - loss: 0.6769 - val_accuracy: 0.5765 - val_loss: 0.6737\n",
      "Epoch 10/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 735ms/step - accuracy: 0.5837 - loss: 0.6782 - val_accuracy: 0.5709 - val_loss: 0.6829\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 735ms/step - accuracy: 0.5837 - loss: 0.6782 - val_accuracy: 0.5709 - val_loss: 0.6829\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 721ms/step - accuracy: 0.5996 - loss: 0.6712\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 721ms/step - accuracy: 0.5996 - loss: 0.6712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cats vs Dogs Test Accuracy: 0.5996045470237732\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE CNN (Cats vs Dogs)\n",
    "catsdogs_history = catsdogs_model.fit(\n",
    "    catsdogs_train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=catsdogs_val_generator\n",
    ")\n",
    "\n",
    "# EVALUATE THE MODEL (Cats vs Dogs)\n",
    "catsdogs_test_loss, catsdogs_test_acc = catsdogs_model.evaluate(catsdogs_test_generator)\n",
    "print(f\"Cats vs Dogs Test Accuracy: {catsdogs_test_acc}\")\n",
    "\n",
    "# SAVE THE IMPROVED MODEL (Cats vs Dogs)\n",
    "catsdogs_model.save('exercise_6_custom_danocup.h5')\n",
    "\n",
    "# SIMPLE INFERENCE SCRIPT (Cats vs Dogs)\n",
    "def catsdogs_predict_image(img_path, model_path='exercise_6_custom_danocup.h5'):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    img = image.load_img(img_path, target_size=catsdogs_img_size)\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    pred = model.predict(img_array)[0,0]\n",
    "    label = \"Dog\" if pred >= 0.5 else \"Cat\"\n",
    "    print(f\"Prediction: {label} (confidence: {pred:.2f})\")\n",
    "\n",
    "# Example usage - Replace with actual image paths:\n",
    "# catsdogs_predict_image(r\"C:\\Users\\LENOVO\\Downloads\\cats_and_dogs\\test_set\\test_set\\cats\\cat.4001.jpg\")\n",
    "# catsdogs_predict_image(r\"C:\\Users\\LENOVO\\Downloads\\cats_and_dogs\\test_set\\test_set\\dogs\\dog.4001.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2e19ed",
   "metadata": {},
   "source": [
    "Muffin vs Chihuahua - ResNet CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d650820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d36efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "MODEL_PATH = 'muffin_vs_chihuahua_model.h5'\n",
    "\n",
    "# test folder\n",
    "PREDICT_DIR = 'predict' \n",
    "\n",
    "IMG_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cff6df9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from muffin_vs_chihuahua_model.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "try:\n",
    "    model = load_model(MODEL_PATH)\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Could not load model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85d8336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'chihuahua' is Class 0\n",
    "# 'muffin' is Class 1\n",
    "CLASS_NAMES = ['Chihuahua', 'Muffin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f382b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Predictions in 'predict' ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\n",
      "[PREDICTION] predict\\c1.jpg\n",
      "  >> Result: Chihuahua\n",
      "  >> Confidence: 81.91%\n",
      "  >> Raw score: 0.1809\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\n",
      "[PREDICTION] predict\\c2.jpg\n",
      "  >> Result: Chihuahua\n",
      "  >> Confidence: 70.98%\n",
      "  >> Raw score: 0.2902\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\n",
      "[PREDICTION] predict\\m1.jpg\n",
      "  >> Result: Muffin\n",
      "  >> Confidence: 85.01%\n",
      "  >> Raw score: 0.1499\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\n",
      "[PREDICTION] predict\\m2.jpg\n",
      "  >> Result: Muffin\n",
      "  >> Confidence: 82.59%\n",
      "  >> Raw score: 0.1741\n",
      "\n",
      "All predictions completed.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Starting Predictions in '{PREDICT_DIR}' ---\")\n",
    "\n",
    "try:\n",
    "    image_files = [f for f in os.listdir(PREDICT_DIR) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    if not image_files:\n",
    "        print(f\"[WARNING] No images found in '{PREDICT_DIR}'.\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n[ERROR] The directory '{PREDICT_DIR}' was not found.\")\n",
    "    print(f\"Please create a folder named 'predict' in your project directory.\")\n",
    "    raise\n",
    "\n",
    "for file_name in image_files:\n",
    "    img_path = os.path.join(PREDICT_DIR, file_name)\n",
    "\n",
    "    try:\n",
    "        \n",
    "        img = image.load_img(img_path, target_size=IMG_SIZE)\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "        img_array = img_array / 255.0 \n",
    "        \n",
    "        prediction = model.predict(img_array)\n",
    "        \n",
    "        raw_score = prediction[0][0]\n",
    "\n",
    "        # result\n",
    "        if raw_score > 0.5:\n",
    "            result_class = CLASS_NAMES[1] # 'Muffin'\n",
    "            confidence = raw_score * 100\n",
    "            raw_score_to_print = 1 - raw_score \n",
    "        else:\n",
    "            result_class = CLASS_NAMES[0] # 'Chihuahua'\n",
    "            confidence = (1 - raw_score) * 100\n",
    "            raw_score_to_print = raw_score \n",
    "\n",
    "        print(f\"\\n[PREDICTION] {img_path}\")\n",
    "        print(f\"  >> Result: {result_class}\")\n",
    "        print(f\"  >> Confidence: {confidence:.2f}%\")\n",
    "        \n",
    "        print(f\"  >> Raw score: {raw_score_to_print:.4f}\") \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Could not process image {img_path}: {e}\")\n",
    "\n",
    "print(\"\\nAll predictions completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
